{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Schrodinger_PINN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SufWleECwnJU",
        "-fE0xGwOwztt",
        "-9Hz3gbuxGVc",
        "swDQncHv0Us8"
      ],
      "mount_file_id": "18gXqrdaEffygRAnBQxq0Txx7UeINx8Mf",
      "authorship_tag": "ABX9TyOD71vk5S+GXyZOo8hqQbR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FMagnani/Physics_Informed_NeuralNetwork/blob/main/Schrodinger_PINN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-cx-bgM3oX"
      },
      "source": [
        "# Setting conda environment - optional  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo1gqYZQ2Q1M"
      },
      "source": [
        "The following must be the first executable cell.  \n",
        "Info: https://colab.research.google.com/drive/1c_RGCgQeLHVXlF44LyOFjfUW32CmG6BP#scrollTo=LAZ11nESX6qt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4V4EdiN1wjf",
        "outputId": "7f6e4a22-4aad-4c11-98eb-08792df14d1f"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:34\n",
            "üîÅ Restarting kernel...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJZEkH7AeEpV"
      },
      "source": [
        "Clone GitHub repo with conda environment configuration file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrGURjY4eLjB",
        "outputId": "8704222b-112c-4533-a342-571bec16a415"
      },
      "source": [
        "!git clone https://github.com/FMagnani/Physics_Informed_NeuralNetwork"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Physics_Informed_NeuralNetwork' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JRXtUZuuihm"
      },
      "source": [
        "We create an environment for the project, with the correct set of packages' versions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7OGuI4Q0hzi",
        "outputId": "c67c5b4b-b19e-4f20-d99d-4bdd77cc6bdb"
      },
      "source": [
        "!conda env create -f Physics_Informed_NeuralNetwork/PINN_env.yml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EnvironmentFileNotFound: '/content/gdrive/MyDrive/PINNs_Data/PINN_env.yml' file not found\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3oYYAce2cNs"
      },
      "source": [
        "Activate environment - source activate is deprecated, but conda activate doesn't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPgdyMHx2dk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9e0223-ad4e-4118-8573-d04e41d78dd6"
      },
      "source": [
        "!source activate PINN"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not find conda environment: PINN\n",
            "You can list all discoverable environments with `conda info --envs`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaB03J57wYty"
      },
      "source": [
        "# Requirements setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45XB_lQ7W6g"
      },
      "source": [
        "Importing Data from GitHub repo.  \n",
        "*This step is not needed if you built the conda environment.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aHT2JnE7INR",
        "outputId": "149c5f7d-21ab-4070-9a3b-020d0ae68b8e"
      },
      "source": [
        "!git clone https://github.com/FMagnani/Physics_Informed_NeuralNetwork"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Physics_Informed_NeuralNetwork'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/195)\u001b[K\rremote: Counting objects:   1% (2/195)\u001b[K\rremote: Counting objects:   2% (4/195)\u001b[K\rremote: Counting objects:   3% (6/195)\u001b[K\rremote: Counting objects:   4% (8/195)\u001b[K\rremote: Counting objects:   5% (10/195)\u001b[K\rremote: Counting objects:   6% (12/195)\u001b[K\rremote: Counting objects:   7% (14/195)\u001b[K\rremote: Counting objects:   8% (16/195)\u001b[K\rremote: Counting objects:   9% (18/195)\u001b[K\rremote: Counting objects:  10% (20/195)\u001b[K\rremote: Counting objects:  11% (22/195)\u001b[K\rremote: Counting objects:  12% (24/195)\u001b[K\rremote: Counting objects:  13% (26/195)\u001b[K\rremote: Counting objects:  14% (28/195)\u001b[K\rremote: Counting objects:  15% (30/195)\u001b[K\rremote: Counting objects:  16% (32/195)\u001b[K\rremote: Counting objects:  17% (34/195)\u001b[K\rremote: Counting objects:  18% (36/195)\u001b[K\rremote: Counting objects:  19% (38/195)\u001b[K\rremote: Counting objects:  20% (39/195)\u001b[K\rremote: Counting objects:  21% (41/195)\u001b[K\rremote: Counting objects:  22% (43/195)\u001b[K\rremote: Counting objects:  23% (45/195)\u001b[K\rremote: Counting objects:  24% (47/195)\u001b[K\rremote: Counting objects:  25% (49/195)\u001b[K\rremote: Counting objects:  26% (51/195)\u001b[K\rremote: Counting objects:  27% (53/195)\u001b[K\rremote: Counting objects:  28% (55/195)\u001b[K\rremote: Counting objects:  29% (57/195)\u001b[K\rremote: Counting objects:  30% (59/195)\u001b[K\rremote: Counting objects:  31% (61/195)\u001b[K\rremote: Counting objects:  32% (63/195)\u001b[K\rremote: Counting objects:  33% (65/195)\u001b[K\rremote: Counting objects:  34% (67/195)\u001b[K\rremote: Counting objects:  35% (69/195)\u001b[K\rremote: Counting objects:  36% (71/195)\u001b[K\rremote: Counting objects:  37% (73/195)\u001b[K\rremote: Counting objects:  38% (75/195)\u001b[K\rremote: Counting objects:  39% (77/195)\u001b[K\rremote: Counting objects:  40% (78/195)\u001b[K\rremote: Counting objects:  41% (80/195)\u001b[K\rremote: Counting objects:  42% (82/195)\u001b[K\rremote: Counting objects:  43% (84/195)\u001b[K\rremote: Counting objects:  44% (86/195)\u001b[K\rremote: Counting objects:  45% (88/195)\u001b[K\rremote: Counting objects:  46% (90/195)\u001b[K\rremote: Counting objects:  47% (92/195)\u001b[K\rremote: Counting objects:  48% (94/195)\u001b[K\rremote: Counting objects:  49% (96/195)\u001b[K\rremote: Counting objects:  50% (98/195)\u001b[K\rremote: Counting objects:  51% (100/195)\u001b[K\rremote: Counting objects:  52% (102/195)\u001b[K\rremote: Counting objects:  53% (104/195)\u001b[K\rremote: Counting objects:  54% (106/195)\u001b[K\rremote: Counting objects:  55% (108/195)\u001b[K\rremote: Counting objects:  56% (110/195)\u001b[K\rremote: Counting objects:  57% (112/195)\u001b[K\rremote: Counting objects:  58% (114/195)\u001b[K\rremote: Counting objects:  59% (116/195)\u001b[K\rremote: Counting objects:  60% (117/195)\u001b[K\rremote: Counting objects:  61% (119/195)\u001b[K\rremote: Counting objects:  62% (121/195)\u001b[K\rremote: Counting objects:  63% (123/195)\u001b[K\rremote: Counting objects:  64% (125/195)\u001b[K\rremote: Counting objects:  65% (127/195)\u001b[K\rremote: Counting objects:  66% (129/195)\u001b[K\rremote: Counting objects:  67% (131/195)\u001b[K\rremote: Counting objects:  68% (133/195)\u001b[K\rremote: Counting objects:  69% (135/195)\u001b[K\rremote: Counting objects:  70% (137/195)\u001b[K\rremote: Counting objects:  71% (139/195)\u001b[K\rremote: Counting objects:  72% (141/195)\u001b[K\rremote: Counting objects:  73% (143/195)\u001b[K\rremote: Counting objects:  74% (145/195)\u001b[K\rremote: Counting objects:  75% (147/195)\u001b[K\rremote: Counting objects:  76% (149/195)\u001b[K\rremote: Counting objects:  77% (151/195)\u001b[K\rremote: Counting objects:  78% (153/195)\u001b[K\rremote: Counting objects:  79% (155/195)\u001b[K\rremote: Counting objects:  80% (156/195)\u001b[K\rremote: Counting objects:  81% (158/195)\u001b[K\rremote: Counting objects:  82% (160/195)\u001b[K\rremote: Counting objects:  83% (162/195)\u001b[K\rremote: Counting objects:  84% (164/195)\u001b[K\rremote: Counting objects:  85% (166/195)\u001b[K\rremote: Counting objects:  86% (168/195)\u001b[K\rremote: Counting objects:  87% (170/195)\u001b[K\rremote: Counting objects:  88% (172/195)\u001b[K\rremote: Counting objects:  89% (174/195)\u001b[K\rremote: Counting objects:  90% (176/195)\u001b[K\rremote: Counting objects:  91% (178/195)\u001b[K\rremote: Counting objects:  92% (180/195)\u001b[K\rremote: Counting objects:  93% (182/195)\u001b[K\rremote: Counting objects:  94% (184/195)\u001b[K\rremote: Counting objects:  95% (186/195)\u001b[K\rremote: Counting objects:  96% (188/195)\u001b[K\rremote: Counting objects:  97% (190/195)\u001b[K\rremote: Counting objects:  98% (192/195)\u001b[K\rremote: Counting objects:  99% (194/195)\u001b[K\rremote: Counting objects: 100% (195/195)\u001b[K\rremote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 195 (delta 56), reused 195 (delta 56), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (195/195), 25.47 MiB | 40.94 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EAamGgR7gcO"
      },
      "source": [
        "data_location = 'Physics_Informed_NeuralNetwork/Data/NLS.mat'"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnX8pjgyv0rz"
      },
      "source": [
        "Install pyDOE.  \n",
        "(I think it's needed even if you created the conda environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlEVzYHV4Mdl",
        "outputId": "2eb9de39-2467-4235-81b6-13a59f4962b6"
      },
      "source": [
        "!pip install pyDOE"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.7/site-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pyDOE) (1.20.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from pyDOE) (1.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgWW26xa7jkF"
      },
      "source": [
        "Import needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAbfqORw7lwh"
      },
      "source": [
        "# For the main script\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.io\n",
        "from pyDOE import lhs \n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "# For the class implementation\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For the LBFGS module\n",
        "import types\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SufWleECwnJU"
      },
      "source": [
        "# LBFGS module  \n",
        "  \n",
        "Adapted from https://github.com/pierremtb/PINNs-TF2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUGAMhaExMxa"
      },
      "source": [
        "# Adapted by https://github.com/pierremtb/PINNs-TF2.0\n",
        "# from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "#import tensorflow as tf\n",
        "#import numpy as np\n",
        "#import time\n",
        "#import types\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "  \n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "  # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "\n",
        "  start_time = time.time()\n",
        "  while nIter < maxIter:\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      # g_old and t variables are not defined yet, but this else will not \n",
        "      # activate in the first run. They're defined later.\n",
        "      y = g - g_old     \n",
        "      s = d*t           \n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "    \n",
        "    # logging iteration time\n",
        "    if (nIter%10==0):\n",
        "      time_elapsed = time.time()-start_time\n",
        "      print('iteration', nIter, '   ',str(time_elapsed)[:4],'sec for 10 it')\n",
        "      start_time = time.time()\n",
        "  \n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fE0xGwOwztt"
      },
      "source": [
        "# Class implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9yq1eKZu_jx"
      },
      "source": [
        "Definitions of the Neural Network class and of all the functions and routines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCHHnkT07ehB"
      },
      "source": [
        "\n",
        "class neural_net(tf.keras.Sequential):\n",
        "    \n",
        "    def __init__(self, ub, lb, layers):\n",
        "        super(neural_net, self).__init__()\n",
        "        \n",
        "#        layers is something like [2, 100, 100, 100, 100, 2]\n",
        "       \n",
        "        tf.keras.backend.set_floatx('float64')\n",
        "        \n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        self.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "\n",
        "        self.add(tf.keras.layers.Lambda(\n",
        "                        lambda X: 2.0*(X-self.lb)/(self.ub-self.lb)-1.0))\n",
        "\n",
        "        for width in layers[1:-1]:\n",
        "            self.add(tf.keras.layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\"))\n",
        "        \n",
        "        self.add(tf.keras.layers.Dense(\n",
        "                layers[-1], activation=None,\n",
        "                kernel_initializer=\"glorot_normal\"))\n",
        "\n",
        "        \n",
        "        # Computing the sizes of weights/biases for future decomposition\n",
        "        self.sizes_w = []\n",
        "        self.sizes_b = []\n",
        "        for i, width in enumerate(layers):\n",
        "            if i != 1:\n",
        "                self.sizes_w.append(int(width * layers[1]))\n",
        "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "\n",
        "    def get_weights(self, convert_to_tensor=True):\n",
        "        w = []\n",
        "        for layer in self.layers[1:]:\n",
        "            weights_biases = layer.get_weights()\n",
        "            weights = weights_biases[0].flatten()\n",
        "            biases = weights_biases[1]\n",
        "            w.extend(weights)\n",
        "            w.extend(biases)\n",
        "        if convert_to_tensor:\n",
        "            w = tf.convert_to_tensor(w)\n",
        "        return w\n",
        "\n",
        "\n",
        "    def set_weights(self, w):\n",
        "        for i, layer in enumerate(self.layers[1:]):\n",
        "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "            weights = w[start_weights:end_weights]\n",
        "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "            weights_biases = [weights, biases]\n",
        "            layer.set_weights(weights_biases)\n",
        "\n",
        "\n",
        "class neural_net_2out(neural_net):\n",
        "    \n",
        "    def __init__(self, ub, lb, layers):\n",
        "        super(neural_net_2out, self).__init__(ub, lb, layers)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        output = super(neural_net_2out, self).call(inputs)\n",
        "        \n",
        "        return output[:,0], output[:,1]\n",
        "\n",
        "\n",
        "\n",
        "class Schrodinger_PINN():\n",
        "    \n",
        "    def __init__(self, x0, u0, v0, x_ub, x_lb, t_b, x_f, t_f, X_star, ub, lb, layers):\n",
        "        \n",
        "        self.model = neural_net_2out(ub, lb, layers)        \n",
        "        \n",
        "        self.x0 = x0\n",
        "        self.t0 = 0*x0\n",
        "        self.u0 = u0\n",
        "        self.v0 = v0\n",
        "        self.x_lb = x_lb\n",
        "        self.x_ub = x_ub\n",
        "        self.t_ub = t_b\n",
        "        self.t_lb = t_b\n",
        "        self.x_f = x_f\n",
        "        self.t_f = t_f\n",
        "        self.X_star = X_star\n",
        "        \n",
        "    \n",
        "    def train(self, n_iterations, optimizer=tf.keras.optimizers.Adam()):\n",
        "    \n",
        "        start_time = time.time()    \n",
        "        #Train step\n",
        "        for _ in tqdm(range(n_iterations)):\n",
        "            self.train_step(optimizer)\n",
        "        elapsed = time.time() - start_time                \n",
        "        print('Training time: %.4f' % (elapsed))\n",
        "    \n",
        "        \n",
        "    def train_step(self, optimizer):\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = self.loss(self.x0,self.t0, self.u0,self.v0)\n",
        " \n",
        "        grads = tape.gradient(loss_value, self.model.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "  \n",
        "        \n",
        "    def loss(self, x0,t0, u0,v0):\n",
        "    \n",
        "        # Loss from supervised learning (at t=0)\n",
        "        X0 = tf.stack([x0, t0], axis=1)\n",
        "        u_pred, v_pred = self.model(X0)\n",
        "        y0 = tf.reduce_mean(tf.square(u0 - u_pred)) + \\\n",
        "             tf.reduce_mean(tf.square(v0 - v_pred))\n",
        "    \n",
        "        # Loss from Schrodinger constraint (at the anchor pts)\n",
        "        f_u, f_v = self.net_f_uv()\n",
        "        yS = tf.reduce_mean(tf.square(f_u)) + \\\n",
        "              tf.reduce_mean(tf.square(f_v))\n",
        "          \n",
        "        # Loss from boundary conditions\n",
        "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb)\n",
        "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub)\n",
        "    \n",
        "        yB = tf.reduce_mean(tf.square(u_lb_pred - u_ub_pred)) + \\\n",
        "             tf.reduce_mean(tf.square(v_lb_pred - v_ub_pred)) + \\\n",
        "             tf.reduce_mean(tf.square(u_x_lb_pred - u_x_ub_pred)) + \\\n",
        "             tf.reduce_mean(tf.square(v_x_lb_pred - v_x_ub_pred))\n",
        "    \n",
        "        return y0 + yS + yB\n",
        "#        return y0\n",
        "\n",
        "\n",
        "    def net_uv(self, x, t):\n",
        "    \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(x)\n",
        "            tape.watch(t)\n",
        "            X = tf.stack([x, t], axis=1) # shape = (N_f,2)\n",
        "        \n",
        "            u, v = self.model(X)\n",
        "         \n",
        "        u_x = tape.gradient(u, x)\n",
        "        v_x = tape.gradient(v, x)\n",
        "\n",
        "        return u, v, u_x, v_x\n",
        "\n",
        "   \n",
        "    def net_f_uv(self):\n",
        "        \n",
        "        x_f = self.x_f\n",
        "        t_f = self.t_f\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:    \n",
        "            tape.watch(x_f)\n",
        "            tape.watch(t_f)\n",
        "            X_f = tf.stack([x_f, t_f], axis=1) # shape = (N_f,2)\n",
        "            \n",
        "            u, v = self.model(X_f)\n",
        "                \n",
        "            u_x = tape.gradient(u, x_f)\n",
        "            v_x = tape.gradient(v, x_f)\n",
        "            \n",
        "        u_t = tape.gradient(u, t_f)\n",
        "        v_t = tape.gradient(v, t_f)        \n",
        "            \n",
        "        u_xx = tape.gradient(u_x, x_f)\n",
        "        v_xx = tape.gradient(v_x, x_f)\n",
        "    \n",
        "        del tape\n",
        "    \n",
        "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v    \n",
        "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
        "    \n",
        "        return f_u, f_v\n",
        "\n",
        "     \n",
        "    def predict(self, x, t):\n",
        "                \n",
        "        with tf.GradientTape(persistent=True) as tape:    \n",
        "            tape.watch(x)\n",
        "            tape.watch(t)\n",
        "            X = tf.stack([x, t], axis=1) # shape = (N_f,2)\n",
        "            \n",
        "            u, v = self.model(X)\n",
        "                \n",
        "            u_x = tape.gradient(u, x)\n",
        "            v_x = tape.gradient(v, x)\n",
        "            \n",
        "        u_t = tape.gradient(u, t)\n",
        "        v_t = tape.gradient(v, t)        \n",
        "            \n",
        "        u_xx = tape.gradient(u_x, x)\n",
        "        v_xx = tape.gradient(v_x, x)\n",
        "    \n",
        "        del tape\n",
        "    \n",
        "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v    \n",
        "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
        "    \n",
        "        return u, v, f_u, f_v\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "class Schrod_PINN_LBFGS(Schrodinger_PINN):\n",
        "    \n",
        "    def __init__(self, x0, u0, v0, x_ub, x_lb, t_b, x_f, t_f, X_star, ub, lb, layers):\n",
        "        super(Schrod_PINN_LBFGS, self).__init__(x0, u0, v0, x_ub, x_lb, t_b, x_f, t_f, X_star, ub, lb, layers)\n",
        "    \n",
        "        # Setting up the optimizers with the hyper-parameters\n",
        "        self.nt_config = Struct()\n",
        "        self.nt_config.learningRate = 1.2\n",
        "        self.nt_config.maxIter = 50\n",
        "        self.nt_config.nCorrection = 50\n",
        "        self.nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
        "    \n",
        "\n",
        "    def train(self, ADAM_iterations, LBFGS_max_iterations=500, optimizer=tf.keras.optimizers.Adam()):\n",
        "            \n",
        "        # ADAM training\n",
        "        if (ADAM_iterations):\n",
        "          super(Schrod_PINN_LBFGS, self).train(ADAM_iterations)\n",
        "            \n",
        "        # LBFGS trainig\n",
        "        if (LBFGS_max_iterations):\n",
        "          self.LBFGS_training(LBFGS_max_iterations)\n",
        "\n",
        "\n",
        "    def LBFGS_training(self, max_iterations):\n",
        "        \n",
        "        self.nt_config.maxIter = max_iterations\n",
        "        \n",
        "        lbfgs(self.loss_and_flat_grad,\n",
        "              self.model.get_weights(),\n",
        "              self.nt_config, Struct())\n",
        "        \n",
        "    \n",
        "    def loss_and_flat_grad(self, w):\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            self.model.set_weights(w)\n",
        "            loss_value = self.loss(self.x0,self.t0, self.u0,self.v0)\n",
        "        grad = tape.gradient(loss_value, self.model.trainable_variables)\n",
        "        grad_flat = []\n",
        "        for g in grad:\n",
        "            grad_flat.append(tf.reshape(g, [-1]))\n",
        "        grad_flat = tf.concat(grad_flat, 0)\n",
        "        return loss_value, grad_flat\n",
        "\n",
        "    \n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j91ihUuxw-Xl"
      },
      "source": [
        "# Main script - model inizialization and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuw6Qyg1vqGa"
      },
      "source": [
        "Data are imported.  \n",
        "An instance of the Neural Network is created, that will be trained in the following cell. In this way, the cell used for the training can be run many times to improve each time the current training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chIGcdrY8Ej-"
      },
      "source": [
        "# Set random seed\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "    \n",
        "# Domain bounds\n",
        "lb = np.array([-5.0, 0.0])\n",
        "ub = np.array([5.0, np.pi/2])\n",
        "\n",
        "N0 = 50     # Number of training pts from x=0\n",
        "N_b = 50    # Number of training pts from the boundaries\n",
        "N_f = 20000 # Number of training pts from the inside - anchor pts\n",
        "\n",
        "  ########################################\n",
        "  ##       DATA PREPARATION             ##\n",
        "  ########################################\n",
        "     \n",
        "# Import data\n",
        "data = scipy.io.loadmat(data_location)\n",
        "\n",
        "    \n",
        "t = data['tt'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = data['uu']\n",
        "Exact_u = np.real(Exact)\n",
        "Exact_v = np.imag(Exact)\n",
        "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
        "    \n",
        "# Creation of the 2D domain\n",
        "X, T = np.meshgrid(x,t)\n",
        "    \n",
        "# The whole domain flattened, on which the final prediction will be made\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact_u.T.flatten()[:,None]\n",
        "v_star = Exact_v.T.flatten()[:,None]\n",
        "h_star = Exact_h.T.flatten()[:,None]\n",
        "    \n",
        "# Choose N0 training points from x and the corresponding u, v at t=0\n",
        "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "x0 = x[idx_x,:]\n",
        "u0 = Exact_u[idx_x,0:1]\n",
        "v0 = Exact_v[idx_x,0:1]\n",
        "    \n",
        "# Choose N_b training points from the time\n",
        "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "tb = t[idx_t,:]\n",
        "    \n",
        "# Latin Hypercube Sampling of N_f points from the interior domain\n",
        "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "# 2D locations on the domain of the boundary training points\n",
        "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
        "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
        "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
        "                   \n",
        "# Recap\n",
        "    \n",
        "# Initial condition pts - the real supervised learning pts\n",
        "# Their 'labels' are u0, v0\n",
        "# shape = (N0,1)\n",
        "x0 = X0[:,0:1]\n",
        "t0 = X0[:,1:2]\n",
        "\n",
        "# Boundary pts used for constraint\n",
        "# shape = (N_b,1)\n",
        "x_lb = X_lb[:,0:1]\n",
        "t_lb = X_lb[:,1:2]\n",
        "\n",
        "x_ub = X_ub[:,0:1]\n",
        "t_ub = X_ub[:,1:2]\n",
        "        \n",
        "# Anchor pts for supervised learning\n",
        "# shape = (N_f,1)\n",
        "x_f = X_f[:,0:1]\n",
        "t_f = X_f[:,1:2]    \n",
        "    \n",
        "# All these are numpy.ndarray with dtype float64\n",
        "    \n",
        "# Conversion to tensors. Recall to WATCH inside a tape\n",
        "x0 = tf.convert_to_tensor(x0[:,0])\n",
        "t0 = tf.convert_to_tensor(t0[:,0])\n",
        "u0 = tf.convert_to_tensor(u0[:,0])\n",
        "v0 = tf.convert_to_tensor(v0[:,0])\n",
        "x_lb = tf.convert_to_tensor(x_lb[:,0])\n",
        "t_lb = tf.convert_to_tensor(t_lb[:,0])\n",
        "x_ub = tf.convert_to_tensor(x_ub[:,0])\n",
        "t_ub = tf.convert_to_tensor(t_ub[:,0])\n",
        "x_f = tf.convert_to_tensor(x_f[:,0])\n",
        "t_f = tf.convert_to_tensor(t_f[:,0])\n",
        "X_star = tf.convert_to_tensor(X_star)\n",
        "\n",
        "\n",
        "layers = [2,100,100,100,100,2]\n",
        "model = Schrod_PINN_LBFGS(x0, u0, v0, x_ub, x_lb, t_ub, x_f, t_f, X_star, ub, lb, layers)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCmhu8_N-OAq"
      },
      "source": [
        "Training. This cell can also be run multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYdlFXcc-Rc6",
        "outputId": "be6b02f3-4806-4635-b625-d4eebc46e833"
      },
      "source": [
        "\n",
        "adam_iterations = 600  # Number of training steps \n",
        "lbfgs_max_iterations = 1200 # Max iterations for lbfgs        \n",
        "\n",
        "# Training\n",
        "model.train(adam_iterations, lbfgs_max_iterations)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/600 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [04:06<00:00,  2.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training time: 246.8419\n",
            "iteration 10     4.050511598587036\n",
            "iteration 20     4.259068250656128\n",
            "iteration 30     4.319369792938232\n",
            "iteration 40     4.369596004486084\n",
            "iteration 50     4.401697635650635\n",
            "iteration 60     4.4434661865234375\n",
            "iteration 70     4.434616804122925\n",
            "iteration 80     4.459354639053345\n",
            "iteration 90     4.424383640289307\n",
            "iteration 100     4.430680513381958\n",
            "iteration 110     4.454207181930542\n",
            "iteration 120     4.45623779296875\n",
            "iteration 130     4.43920636177063\n",
            "iteration 140     4.449477910995483\n",
            "iteration 150     4.458633661270142\n",
            "iteration 160     4.439203262329102\n",
            "iteration 170     4.435734510421753\n",
            "iteration 180     4.4422454833984375\n",
            "iteration 190     4.440509796142578\n",
            "iteration 200     4.452229738235474\n",
            "iteration 210     4.452256917953491\n",
            "iteration 220     4.44450306892395\n",
            "iteration 230     4.442287445068359\n",
            "iteration 240     4.430931568145752\n",
            "iteration 250     4.425796270370483\n",
            "iteration 260     4.43603515625\n",
            "iteration 270     4.436618328094482\n",
            "iteration 280     4.462127208709717\n",
            "iteration 290     4.44105076789856\n",
            "iteration 300     4.44102931022644\n",
            "iteration 310     4.432899475097656\n",
            "iteration 320     4.459903717041016\n",
            "iteration 330     4.450834274291992\n",
            "iteration 340     4.4401514530181885\n",
            "iteration 350     4.441687822341919\n",
            "iteration 360     4.443289756774902\n",
            "iteration 370     4.461451530456543\n",
            "iteration 380     4.431509971618652\n",
            "iteration 390     4.45341157913208\n",
            "iteration 400     4.440454959869385\n",
            "iteration 410     4.449212074279785\n",
            "iteration 420     4.429672718048096\n",
            "iteration 430     4.438821315765381\n",
            "iteration 440     4.444490671157837\n",
            "iteration 450     4.457867622375488\n",
            "iteration 460     4.437531232833862\n",
            "iteration 470     4.44924521446228\n",
            "iteration 480     4.439006805419922\n",
            "iteration 490     4.449163913726807\n",
            "iteration 500     4.4347310066223145\n",
            "iteration 510     4.430172443389893\n",
            "iteration 520     4.461280822753906\n",
            "iteration 530     4.455652475357056\n",
            "iteration 540     4.455623388290405\n",
            "iteration 550     4.445554256439209\n",
            "iteration 560     4.439131259918213\n",
            "iteration 570     4.439272165298462\n",
            "iteration 580     4.425352573394775\n",
            "iteration 590     4.45211935043335\n",
            "iteration 600     4.4433910846710205\n",
            "iteration 610     4.448535680770874\n",
            "iteration 620     4.437279939651489\n",
            "iteration 630     4.442873954772949\n",
            "iteration 640     4.451902866363525\n",
            "iteration 650     4.441904306411743\n",
            "iteration 660     4.417766571044922\n",
            "iteration 670     4.443082332611084\n",
            "iteration 680     4.4285008907318115\n",
            "iteration 690     4.419154644012451\n",
            "iteration 700     4.422595500946045\n",
            "iteration 710     4.4304046630859375\n",
            "iteration 720     4.4738242626190186\n",
            "iteration 730     4.450838088989258\n",
            "iteration 740     4.443972110748291\n",
            "iteration 750     4.458138465881348\n",
            "iteration 760     4.44071364402771\n",
            "iteration 770     4.438821792602539\n",
            "iteration 780     4.433420658111572\n",
            "iteration 790     4.421428918838501\n",
            "iteration 800     4.438973903656006\n",
            "iteration 810     4.430969476699829\n",
            "iteration 820     4.436779975891113\n",
            "iteration 830     4.438514709472656\n",
            "iteration 840     4.438655138015747\n",
            "iteration 850     4.44371771812439\n",
            "iteration 860     4.451002836227417\n",
            "iteration 870     4.437641382217407\n",
            "iteration 880     4.453229904174805\n",
            "iteration 890     4.45775032043457\n",
            "iteration 900     4.421401500701904\n",
            "iteration 910     4.440619945526123\n",
            "iteration 920     4.440711975097656\n",
            "iteration 930     4.435873508453369\n",
            "iteration 940     4.443731784820557\n",
            "iteration 950     4.430922031402588\n",
            "iteration 960     4.422433853149414\n",
            "iteration 970     4.450613737106323\n",
            "iteration 980     4.420928955078125\n",
            "iteration 990     4.424656867980957\n",
            "iteration 1000     4.428548812866211\n",
            "iteration 1010     4.423346281051636\n",
            "iteration 1020     4.441743850708008\n",
            "iteration 1030     4.424675703048706\n",
            "iteration 1040     4.436923503875732\n",
            "iteration 1050     4.421209096908569\n",
            "iteration 1060     4.423921346664429\n",
            "iteration 1070     4.434671878814697\n",
            "iteration 1080     4.425827503204346\n",
            "iteration 1090     4.4268410205841064\n",
            "iteration 1100     4.443259239196777\n",
            "iteration 1110     4.4250969886779785\n",
            "iteration 1120     4.432626247406006\n",
            "iteration 1130     4.436513662338257\n",
            "iteration 1140     4.425207138061523\n",
            "iteration 1150     4.416385889053345\n",
            "iteration 1160     4.425017833709717\n",
            "iteration 1170     4.421326637268066\n",
            "iteration 1180     4.420655250549316\n",
            "iteration 1190     4.446439504623413\n",
            "iteration 1200     4.179190635681152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75aih_2l-mvq"
      },
      "source": [
        "Final prediction.  \n",
        "That's done on the full domain, from which we sampled the training data and the points on which to check the physics constraints.  \n",
        "X_star -the full domain- can be treated as the *validation data*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1heCzHx-oVd",
        "outputId": "b39cf927-893b-45bb-ceb0-963499d25177"
      },
      "source": [
        "# final prediction\n",
        "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star[:,0], X_star[:,1])\n",
        "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
        "                \n",
        "# final error\n",
        "u_pred = tf.reshape(u_pred, shape=(51456,1))\n",
        "v_pred = tf.reshape(v_pred, shape=(51456,1))\n",
        "h_pred = tf.reshape(h_pred, shape=(51456,1))\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error v: %e' % (error_v))\n",
        "print('Error h: %e' % (error_h))\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Error u: 2.954596e-01\n",
            "Error v: 4.730112e-01\n",
            "Error h: 7.173063e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Hz3gbuxGVc"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCFYzDSXxI4w"
      },
      "source": [
        "Plotting of the figure.  \n",
        "It shows the prediction on the full domain and three time snapshots showing the comparison of the simulation with the ground truth.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73gSULAXxID6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "42101a0d-d152-4f6d-ccba-8bec8e4d0a8b"
      },
      "source": [
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################    \n",
        "\n",
        "# Fix for some random bug happening in griddata if all this stuff is not writeable.\n",
        "X_star_np = X_star.numpy()\n",
        "h_pred_np = h_pred.numpy()\n",
        "X_star_np.flags.writeable=True\n",
        "h_pred_np.flags.writeable=True\n",
        "\n",
        "\n",
        "H_pred = griddata(X_star_np, h_pred_np[:,0], (X, T), method='cubic')\n",
        "\n",
        "\n",
        "X0 = tf.stack([x0,0*x0],axis=1) # (x0, 0)\n",
        "X_lb = tf.stack([0*tb + lb[0], tb], axis=1) # (lb[0], tb)\n",
        "X_ub = tf.stack([0*tb + ub[0], tb], axis=1) # (ub[0], tb)\n",
        "X_u_train = np.vstack([X0, X_lb[:,:,0], X_ub[:,:,0]])\n",
        "    \n",
        " \n",
        "###########  h(t,x)  ##################    \n",
        "    \n",
        "fig1, ax1 = plt.subplots(1,1)\n",
        "    \n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "ax1 = plt.subplot(gs0[:, :])\n",
        "    \n",
        "h = ax1.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
        "              extent=[lb[1], ub[1], lb[0], ub[0]], \n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax1)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig1.colorbar(h, cax=cax)\n",
        "    \n",
        "ax1.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "    \n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax1.plot(t[75]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax1.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
        "ax1.plot(t[125]*np.ones((2,1)), line, 'k--', linewidth = 1)    \n",
        "    \n",
        "ax1.set_xlabel('$t$')\n",
        "ax1.set_ylabel('$x$')\n",
        "ax1.set_title('$|h(t,x)|$', fontsize = 10)\n",
        "    \n",
        " \n",
        "########   h(t,x) slices ##################    \n",
        " \n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')    \n",
        "ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact_h[:,100], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])\n",
        "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)\n",
        "    \n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$|h(t,x)|$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-5.1,5.1])\n",
        "ax.set_ylim([-0.1,5.1])    \n",
        "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)\n",
        "    \n",
        "    \n",
        " "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '$t = 0.98$')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swDQncHv0Us8"
      },
      "source": [
        "# Save figure and weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6EA_aEmDmQf"
      },
      "source": [
        "Save figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI58FzYMDrCE"
      },
      "source": [
        "fig1.savefig('final_results')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkwuB_WLxOzi"
      },
      "source": [
        "*model.model.save* would save the model in a format including the Network architecture.  \n",
        "*model.model.save_weights* saves the current weights only. They can be loaded by a model that has exactly the same implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKgOvt9vlz7A"
      },
      "source": [
        "model.model.save_weights('./checkpoints/trained_weights')\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cCcpbywb8MF"
      },
      "source": [
        "Zip the folder with weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3SsvW1Hae3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015ee4b5-58b4-4413-f955-7346d04bc285"
      },
      "source": [
        "!zip -r /content/check.zip /content/checkpoints"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/checkpoints/ (stored 0%)\n",
            "  adding: content/checkpoints/trained_weights.data-00000-of-00001 (deflated 5%)\n",
            "  adding: content/checkpoints/trained_weights.index (deflated 56%)\n",
            "  adding: content/checkpoints/checkpoint (deflated 39%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUXDgouqcDg-"
      },
      "source": [
        "Downloads the zipped folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5HxzvclbI3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c0001ed1-41ba-4483-8c32-4d9b95a8b281"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/check.zip\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e77fd2ba-5388-4a00-8bdb-37f17f83a872\", \"check.zip\", 237253)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}